>>> df2016 = spark.read.csv("/mnt/c/Users/sagar/Desktop/Temp/nyc-parking-tickets/Yearly/Parking_Violations_Issued_-_Fiscal_Year_2016.csv", inferSchema=True, header=True)
>>> df2017 = spark.read.csv("/mnt/c/Users/sagar/Desktop/Temp/nyc-parking-tickets/Yearly/Parking_Violations_Issued_-_Fiscal_Year_2017.csv", inferSchema=True, header=True)
>>> df2018 = spark.read.csv("/mnt/c/Users/sagar/Desktop/Temp/nyc-parking-tickets/Yearly/Parking_Violations_Issued_-_Fiscal_Year_2018.csv", inferSchema=True, header=True)
>>> dfFull = df2016.union(df2017)
>>> dfFull = dfFull.union(df2018)
>>> dfFull = dfFull.sample(False,0.1,0)
>>> dfFull.count()
>>> dfFull.repartition(1).write.csv(path="/mnt/c/Users/sagar/Desktop/Temp/nyc-parking-tickets
/FullDataParkingNYC.csv", mode="append", header="true")
>>> dfFull.count()