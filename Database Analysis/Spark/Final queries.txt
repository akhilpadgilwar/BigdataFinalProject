Using Python version 3.6.7 (default, Oct 22 2018 11:32:17)
SparkSession available as 'spark'



###################### Number of Parking Violation By NY state and Others #######################

import sys
from pyspark import SparkConf, SparkContext
from csv import reader
line1 = sc.textFile("/mnt/c/Users/akhil/Desktop/nyc-parking-tickets/FullParkingNYCViolationsData.csv")
line1 = line1.mapPartitions(lambda x: reader(x))
state = line1.map(lambda x: (("NY" if str(x[2]) =="NY" else "Other" ),1)).reduceByKey(lambda x, y: x + y)
state.take(10)
[('Other', 722233), ('NY', 2594142)]



######################### Number of Parking Violation By vehicle Make #########################



import sys
from pyspark import SparkConf, SparkContext
from csv import reader
id3 = line1.map(lambda x: ((x[7]),1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)
VehicleMake = sc.parallelize(id3.take(10)).map(lambda x: (x[0], x[1]))
VehicleMake.take(10)

[('FORD', 1327031), ('TOYOT', 1210979), ('HONDA', 1076000), ('NISSA', 906588), ('CHEVR', 734740), ('FRUEH', 460533), ('ME/BE', 393382), ('DODGE', 373968), ('BMW', 369649), ('JEEP', 339538)]

#################### Number of Parking Violation By Registration State #############################

import sys
from pyspark import SparkConf, SparkContext
from csv import reader
id4 = line1.map(lambda x: ((x[2]),1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)
violationcodesByState = sc.parallelize(id4.take(10)).map(lambda x: (x[0], x[1]))
violationcodesByState.take(10)
[('NY', 8557058), ('NJ', 953792), ('PA', 279128), ('CT', 145302), ('FL', 142670), ('IN', 101050), ('MA', 88946), ('VA', 72148), ('MD', 61451), ('NC', 55173)]




########### Number of Parking Violation By Plate ID and Number of Parking Violation count ################

import sys
from pyspark import SparkConf, SparkContext
from csv import reader
id1 = line1.map(lambda x: ((x[1]),1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)
topviolator1 = sc.parallelize(id1.take(5)).map(lambda x: (x[0], x[1]))
topviolator1.take(5)
[('BLANKPLATE', 12365), ('12359MG', 1040), ('96087MA', 936), ('2028685', 896), ('47603MD', 885)]



#################### Number of Parking Violation By Street and Number of Parking Violation count ###########

import sys
from pyspark import SparkConf, SparkContext
from csv import reader
id2 = line1.map(lambda x: ((x[24]),1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)
 topviolatorByStreet = sc.parallelize(id2.take(10)).map(lambda x: (x[0], x[1]))
topviolatorByStreet.take(10)

[('Broadway', 219449), ('3rd Ave', 165460), ('5th Ave', 102526), ('Madison Ave', 99256), ('Lexington Ave', 81677), ('2nd Ave', 80991), ('1st Ave', 71743), ('7th Ave', 68935), ('8th Ave', 62479), ('Queens Blvd', 59834)]




